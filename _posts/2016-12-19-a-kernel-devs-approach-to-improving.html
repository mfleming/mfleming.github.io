---
layout: post
title: A Kernel Dev's Approach to Improving Mutt's Performance - Part 1
date: '2016-12-19T06:14:00.000-08:00'
tags: 
modified_time: '2016-12-19T06:14:10.146-08:00'
blogger_id: tag:blogger.com,1999:blog-5657688967837431090.post-4869255022418161982
blogger_orig_url: http://www.codeblueprint.co.uk/2016/12/a-kernel-devs-approach-to-improving.html
---

A large chunk of my day is spent sifting through my inbox -- mail, and
especially patches, drive every task I work on. I use mutt to read all
my mail, and its mark-thread-read functionality is a godsend for
mailing lists. It allows me to quickly process unread mail and ignore
those threads that I don't have any interest in.<br /><br />But I've
got a confession: I don't actually open the mail folder containing
linux-kernel mailing list emails anymore. I did years ago, but since
LKML started receiving over 400 emails a day (my folder is 6.4GB),
I've given up.<br /><br />There are two reasons:<br /><br />&nbsp; 1.
Opening the folder takes over a minute<br />&nbsp; 2. Mutt locks up
when the LKML folder is open and new mail arrives<br /><br />Instead,
over time, I've constructed a bunch of Gmail filters to siphon off
those LKML mails that I actually want to read. And that works OK, but
I still miss interesting threads for which I have no prior filters
configured. That bugs me. I finally decided to do something about it
and dig into why mutt was performing so poorly.<br /><br />I'm only
going to discuss item 1. above in this post. In part 2, I'll
investigate the lockup. In addition, this post is restricted to tuning
mutt's performance via config options, which is basically the lowest
hanging fruit.<br /><br /><blockquote><i>It turns out that a lot of
the tunings discussed below are described in <a
href="http://www.mutt.org/doc/manual/#tuning" target="_blank">mutt's
documentation</a>. I didn't read that documentation beforehand, and
besides, the following techniques demonstrate how to tackle
performance issues from first principles. The fact that they turned up
items that were discussed in the official mutt documentation goes to
show how well they work!</i></blockquote><h4>A reproduction
case</h4>First, we need a simple way to reproduce the issue. Handily,
mutt provides a way to open a maildir folder on startup (you do need
to hit the 'q' key once mutt is running),<br /><br /><pre>    $ time -p mutt -f ~/Maildir/.lkml<br />    real 84.88<br />    user 31.56<br />    sys 19.07<br /></pre>Our baseline for opening the LKML folder is around 84 seconds. Armed with this, we can dive into mutt's performance.<br /><br /><h4>Checking System-wide Health</h4>It's a good idea to look at the state of the entire system before delving into application-specific issues. For example, if the system is swapping (and that would be bad) that'll show up with system-wide statistics.<br /><br /><div><div>Running <code>`vmstat 1`</code> while opening the LKML folder shows that cache is slowly increasing,</div><div><br /></div><div><pre>    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----<br />     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st<br />     1  0      0 4761080   2316 1994984    0    0 55392  3268 7571 30468  9 7 72 13  0<br />     1  0      0 4669444   2316 2064456    0    0 61876     0 8106 32759 9  8 72 12  0<br />     1  0      0 4561688   2316 2146792    0    0 73448     0 9334 38210 10  7 73 10  0<br />     0  1      0 4478980   2316 2213772    0    0 59488  3264 7925 32041 7  8 72 12  0<br />     0  1      0 4382508   2316 2285936    0    0 63856     0 8956 34934 10  7 72 11  0<br />     2  1      0 4297220   2316 2349256    0    0 56128 23504 9099 37290 14 10 63 13  0<br /></pre></div><div><br /></div><div>And once we're done we see,</div><div><br /></div><div><pre>    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----<br />     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st<br />     0  0      0  60472    324 5035292    0    0     0     0   87  222  0 0 100  0  0<br />     0  0      0  60472    324 5035288    0    0     0     0   81  185  0 0 100  0  0<br />     0  0      0  60472    324 5035288    0    0     0     0   88  210  0 0 100  0  0<br />     0  0      0  60472    324 5035288    0    0     0     0   80  186  0 0 100  0  0<br /></pre></div><div><br /></div><div>That's only ~60MB in the free column. The majority is allocated to the page cache. Additionally the block-in (bi) and block-out (bo) columns indicate that the system is reading/writing to disk. Which is totally expected when opening a mail folder.</div><div><br /></div><div>From a system health perspective, this looks fine. We're not swapping, we're not trying to run loads of tasks -- we're not seeing any kind of saturation of resources (CPU, memory, etc).</div><div><br /></div><div><h4>Breaking things down at the application level</h4></div><div>Using <code>`pidstat -C mutt 1`</code> shows that roughly half of the time is spent on the CPU and the rest spent waiting for disk I/O (here's a sample from the middle of a trace):</div><div><br /></div><div><pre>    21:53:07      UID       PID    %usr %system  %guest    %CPU   CPU  Command<br />    21:53:08     1000      4475   31.00   22.00    0.00   53.00     0  mutt<br /></pre></div><div><br />Note that things are simplified enormously when parsing the <code>%CPU</code> column because mutt isn't a threaded application.<br /><br /></div><div>The mean CPU usage from start to finish is 52.7%, which means mutt spent roughly half of its time on-CPU and half off-CPU. The time spent off-CPU was probably waiting for I/O to complete, given the above vmstat output.<br /><br /></div></div><div><div>But we don't need to guess, we can confirm that by summing the delays reported by the <code>sched_stat_iowait</code> tracepoint. A typical entry looks like this,</div><div><br /></div><div><pre>    kworker/u8:5-7166  [001] d..4  78.852761: sched_stat_iowait: comm=mutt pid=7200 delay=804910 [ns]<br /></pre></div><div><br /></div><div>Using a bit of sed, grep and awk we can sum these delays and print the total delay in seconds,</div><div><br /></div><div><pre>    grep -E "iowait.*mutt" trace  | sed -e 's/.*delay=//' | \<br />        awk 'BEGIN{s=0}{s += $1} END{printf "%.2fs\n", s/1000000000}'<br /></pre></div><div><br /></div><div>Which is 32.62s, and that's only 38% of the 84 seconds runtime recorded by pidstat. The rest of the off-CPU time was probably caused by sleeping on mutexes or waitqueues, etc in the kernel.</div><div><br /></div><div>Clearly, we need to be figuring out what mutt is doing when it's on-CPU executing instructions since that is where the majority (62%) of time is going.</div><div><br /></div><div>Using perf record we can get that info. Here are all the functions that took more than 1% of the runtime,</div><div><br /></div><div><pre>    $ perf report -g fractal --no-children<br /><br />    +    5.07%  mutt     libc-2.24.so        [.] __strchr_sse2<br />    +    3.23%  mutt     libc-2.24.so        [.] malloc_consolidate<br />    +    3.15%  mutt     [kernel.kallsyms]   [k] _xfs_buf_find<br />    +    3.09%  mutt     libc-2.24.so        [.] _int_malloc<br />    +    2.78%  mutt     libc-2.24.so        [.] __strspn_sse42<br />    +    1.46%  mutt     libc-2.24.so        [.] _int_free<br />    +    1.43%  mutt     libc-2.24.so        [.] strlen<br />    +    1.32%  mutt     [kernel.kallsyms]   [k] kmem_cache_alloc<br />    +    1.17%  mutt     [kernel.kallsyms]   [k] entry_SYSCALL_64<br />    +    1.10%  mutt     mutt                [.] strchr@plt<br />    +    1.06%  mutt     libc-2.24.so        [.] _IO_getline_info<br /></pre></div><div><br /></div><div>Mutt is doing some string operations, something with <code>malloc(3)</code> and when I expanded the <code>_xfs_buf_find()</code> call stack it showed that it is hit mainly from the <code>stat(2)</code> syscall path.</div><div><br /></div><div>Turning on the <code>--children</code> option rebuilding mutt with frame pointers (<code>-fno-omit-frame-pointer</code>) gives us more mutt symbols in the profile,</div><div><br /></div><div><pre>  Children      Self  Command  Shared Object       Symbol<br />    36.85%     0.32%  mutt     [kernel.kallsyms]   [k] entry_SYSCALL_64_fastpath<br />    36.20%     0.00%  mutt     mutt                [.] main<br />    36.20%     0.00%  mutt     libc-2.24.so        [.] __libc_start_main<br />    36.20%     0.00%  mutt     [unknown]           [k] 0x26c6258d4c544155<br />    34.98%     0.00%  mutt     mutt                [.] mx_open_mailbox<br />    34.95%     0.00%  mutt     mutt                [.] mh_read_dir<br />    34.95%     0.00%  mutt     mutt                [.] maildir_read_dir<br />    33.72%     0.22%  mutt     mutt                [.] maildir_delayed_parsing<br />    29.60%     0.62%  mutt     mutt                [.] mutt_read_rfc822_header<br />    20.91%     0.68%  mutt     mutt                [.] mutt_parse_rfc822_line<br />    17.62%     0.03%  mutt     libc-2.24.so        [.] __GI___libc_open<br />    17.49%     0.01%  mutt     [kernel.kallsyms]   [k] sys_open<br /></pre></div><div><br /></div><div>The majority of the time is spent in the following call path,</div><div><br /></div><div><pre>-   36.20%     0.00%  mutt     mutt                [.] main<br />   - main<br />      - 96.64% mx_open_mailbox<br />         - 99.90% maildir_read_dir<br />            - mh_read_dir<br />               - 96.47% maildir_delayed_parsing<br />                  + 88.32% mutt_read_rfc822_header<br /></pre></div><div><br /></div><div>Reading the mutt source code, it looked like <code>maildir_delayed_parsing()</code>opens <b>*every*</b> file in a maildir and reads the header contents. Given that my LKML directory size is 6.4GB, that's gotta be expensive. Plus that seemed a strange kind of thing to do every time I open a mail folder since the headers shouldn't be changing. It would make far more sense to cache those headers somewhere rather than reading and parsing them every time.</div></div><div><div><br /></div><div>With that in mind, while reading the mutt source I noticed that there are references to a "hcache" or "header cache" in <code>maildir_delayed_parsing()</code>. That sounded exactly like something that would help improve the speed of opening the LKML folder.  Sure enough, there's a <code>header_cache</code> config variable in my muttrc that was commented out and therefore, disabled. Uncommenting it showed,</div><div><br /></div><div><pre>Error in /home/matt/.muttrc, line 1606: header_cache: unknown variable<br />source: errors in /home/matt/.muttrc<br />Press any key to continue...</pre></div><div><br /></div><div>Rebuilding mutt with header cache support and turning on the <code>header_cache</code> config option speeds things up about 3x from 79.48s to 22.04s!</div><div><br /><h4>Mutt's header cache</h4></div><div>At this point, I'm wondering whether we can do better by using a different database for the hcache. There are four available: tokyocabinet, qdbm, gdbm and Berkeley4 (the default).</div><div><br /></div><div>I couldn't find QDBM for openSUSE, but it's a sourceforge project that hasn't been updated since 2013. I didn't hold much hope for it.</div><div><br /></div><div>Here are the choices of database and the size after initialisation, along with the time to open the LKML folder when the header cache is enabled. I'm ignoring the initialisation times because that's a one-time operation and frankly, I'd be willing to put up with waiting minutes to create the database if it shaved off a few seconds every time I opened the maildir folder.</div><div><br /></div><div><table border="1"><tbody><tr><td>DB</td><td>Time (sec)</td><td>DB Size (MB)</td></tr><tr><td>Berkeley4</td><td>22.04</td><td>1620</td></tr><tr><td>tokyocabinet</td><td>24.04</td><td>239</td></tr><tr><td>gdbm</td><td>22.03</td><td>855</td></tr></tbody></table></div><div><br /></div><div>In terms of database size and speed, gdbm is the winner, but only marginally over the default, Berkeley4. There is clearly a cost for the compression of the tokyocabinet file.</div><div><br /></div><div>Doing on-CPU vs off-CPU analysis again, we're now spending even <b>*more*</b>time doing CPU intensive operations. The call stack from a perf record profile now looks quite different,</div></div><div><div><br />
</div><div><pre>-   23.05%     0.00%  mutt     mutt                [.] main<br />  - main<br />      - 80.51% mx_open_mailbox<br />         - 60.39% maildir_read_dir<br />            - mh_read_dir<br />               - 77.50% maildir_delayed_parsing<br />                  - 52.07% mutt_hcache_restore<br />                     - 65.97% restore_address<br />                        + 32.93% _int_malloc<br />                        + 31.92% restore_char<br />                        + 15.55% __libc_calloc<br />                          11.21% safe_calloc<br />                          4.42% __memmove_avx_unaligned_erms<br />                          1.51% safe_malloc<br />                          1.16% memcpy@plt<br />                          1.14% calloc@plt<br />                     + 9.49% restore_char<br />                     + 5.65% restore_list<br />                     + 3.23% mutt_new_body<br />                       3.03% _int_malloc<br />                     + 2.69% mutt_free_header<br />                       1.72% __libc_calloc<br />                       1.67% safe_calloc<br />                       1.27% __memmove_avx_unaligned_erms<br />                       1.14% strlen<br />                     + 0.63% safe_strdup<br />                       0.63% safe_free<br /></pre></div><div><br /></div><div>That's 80% of the time opening the maildir. Digging into
<code>maildir_delayed_parsing()</code> shows that we're spending a lot
of time just calling <code>malloc(3)</code>.</div><div><br
/><h4>Mutt's progress bar</h4></div><div>Before going too far down the
road of analysing <code>malloc(3)</code> and friends, a function
called <code>maildir_parse_dir()</code> caught my eye.<br /><br /></div><div><pre>             - 10.66% maildir_parse_dir<br />                  + 49.14% mutt_progress_update<br />                    9.14% _int_malloc<br />                  + 8.81% maildir_parse_flags<br />                    6.30% __libc_calloc<br />                    5.18% strrchr<br />                    4.38% safe_calloc<br />                    2.83% __strncmp_sse42<br />                  + 2.22% safe_strdup<br />                    2.12% strlen<br />                    1.22% _int_free<br />                    1.22% calloc@plt<br />                    1.11% __memmove_avx_unaligned_erms<br />                    1.09% free<br />                    0.73% free@plt<br />                    0.72% mutt_strncmp<br />                    0.60% mutt_clear_error.part.1<br />                    0.51% safe_free<br /></pre></div><div><br /></div><div>It looks as though nearly 50% of <code>maildir_parse_dir()</code>'s duration is spent updating the progress bar. Let's see if we can disable that in the config. Searching in the muttrc for the word "progress" eventually turns up the <code>read_inc</code> option, which controls how mutt displays progress updates when opening mailboxes,</div><div><br /></div><div><pre># set read_inc=10<br />#<br /># Name: read_inc<br /># Type: number<br /># Default: 10<br />#<br />#<br /># If set to a value greater than 0, Mutt will display which message it<br /># is currently on when reading a mailbox or when performing search actions<br /># such as search and limit. The message is printed after<br /># read_inc messages have been read or searched (e.g., if set to 25, Mutt will<br /># print a message when it is at message 25, and then again when it gets<br /># to message 50).  This variable is meant to indicate progress when<br /># reading or searching large mailboxes which may take some time.<br /># When set to 0, only a single message will appear before the reading<br /># the mailbox.<br />#<br /># Also see the ``$write_inc'' variable and the ``Tuning'' section of the<br /># manual for performance considerations.<br /></pre></div><div><br /></div><div>Disabling it by setting it to zero shaves off almost 2 seconds from the open time. We're now at 20.95s.</div><div><br /></div><div>Well, that was an easy win.</div><div><br /></div><div>Given that we've changed things, it's important to re-record the CPU profile. <code>maildir_parse_dir()</code> has moved further down the hottest function list.</div><div><br /></div><div><pre>-   24.78%     0.00%  mutt     mutt                [.] main<br />   - main<br />      - 78.87% mx_open_mailbox<br />         - 54.76% maildir_read_dir<br />            - mh_read_dir<br />               + 78.97% maildir_delayed_parsing<br />               + 11.80% maildir_add_to_context<br />               + 5.56% maildir_parse_dir<br />               + 1.73% maildir_free_maildir<br />               + 0.61% __xstat64<br />         + 45.24% mutt_sort_headers<br />      + 21.08% mutt_index_menu<br /></pre></div><div><br /></div><div>I <b>*still*</b> don't want to delve into the <code>malloc(3)</code> calls yet, so let's look at <code>maildir_add_to_context()</code>.</div><div><br /></div><div><pre>               - 11.80% maildir_add_to_context<br />                  - 96.08% mx_update_context<br />                     - 96.80% crypt_query<br />                        - 92.48% mutt_is_application_pgp<br />                             60.52% ascii_strcasecmp<br />                           - 39.48% mutt_get_parameter<br />                                ascii_strcasecmp<br />                        + 4.30% mutt_is_multipart_signed<br />                          1.62% ascii_strcasecmp<br />                        + 1.07% mutt_is_multipart_encrypted<br />                          0.54% mutt_get_parameter<br />                       3.20% mutt_score_message<br />                  + 2.42% mx_alloc_memory<br />                    0.94% mutt_score_message<br /></pre></div><div><br /></div><div>Hmm.. I don't actually use the crypto support. Let's see what a difference it makes if I configure mutt without pgp or smime support. It turns out not much, it still takes about 20 seconds. I might as well leave those config options enabled. We've obviously reached diminishing returns this far down the call path. The next choice is between <code>maildir_read_dir()</code> (54.76%) and <code>mutt_sort_headers()</code> (45.24%). The former concerns memory allocation and sorting maildir files and the latter sorting mutt threads and hashing.</div><div><br /></div><div>Let's investigate both before making a decision on which to drill down further.</div><div><br /></div><div>We already know from our previous analysis that <code>malloc(3)</code> plays a large role in this function's call stack. <code>mutt_hcache_restore()</code> -&gt; <code>restore_envelope()</code> -&gt; <code>restore_address()</code> looks like where most of the CPU time is going.</div><div><br /></div><div><code>restore_address()</code> allocates memory to hold the rfc822 address from the mail file. Allocating these small objects for every file in the maildir incurs a huge overhead both in memory (every object has associated metadata) and CPU time. Most of the time, there's only one address to restore, so we could optimise that common case, and allocate more buffers if necessary. But the way that the rfc822 address code works means you need to read sizes from the db before allocating data. Not a huge amount of work to optimise, but more than I'd like to do right now.</div><div><br /><h4>Mutt's header cache verification</h4></div><div>Going back up the call stack and re-reading <code>maildir_delayed_parsing()</code>, I noticed this snippet of code,</div><br /><div><pre>#if USE_HCACHE<br />    if (option(OPTHCACHEVERIFY))<br />    {<br />       ret = stat(fn, &amp;lastchanged);<br />    }<br />    else<br />    {<br />      lastchanged.st_mtime = 0;<br />      ret = 0;<br />    }</pre></div><div>It turns a whole load of <code>stat(2)</code> calls were caused by having the <code>maildir_header_cache_verify</code> config option enabled. Disabling header cache verification reduces time to 8.9s! <br /><br /><h4>Mutt's strict threads</h4></div><div>After this, nothing stood out as being obviously tunable. So I started looking at <code>mutt_sort_headers()</code> and I saw this,</div><br /><div><pre>if (!option (OPTSTRICTTHREADS))<br />    pseudo_threads (ctx);</pre></div><div>Which is described in the mutt config file as,</div></div><div><div><br /></div><div><pre># Name: strict_threads<br /># Type: boolean<br /># Default: no<br />#<br />#<br /># If set, threading will only make use of the ``In-Reply-To'' and<br /># ``References'' fields when you ``$sort'' by message threads.  By<br /># default, messages with the same subject are grouped together in<br /># ``pseudo threads.''. This may not always be desirable, such as in a<br /># personal mailbox where you might have several unrelated messages with<br /># the subject ``hi'' which will get grouped together. See also<br /># ``$sort_re'' for a less drastic way of controlling this<br /># behaviour.<br /></pre></div><div><br /></div><div>Calculating the hash of a pseudo thread's subject is quite expensive, especially if we don't really care about grouping poorly titled emails together. Disabling it reduces the open time to 8.49s.<br /><br /><h4>Tuning list</h4>Here's the list of tunings that got a 9x improvement in the time for mutt to open my LKML folder:<br /><ol><li>Enable mutt's header cache, <code>header_cache="path"</code> (3.8x)</li><li>Disable mutt's progress bar, <code>read_inc=0</code> (0.2x)</li><li>Disable header cache verification, <code>maildir_header_cache_verify=no</code> (5x)</li></ol>I think that's a good place to stop (I'm out of useful config options anyway). I don't mind waiting for 8 seconds or so for my LKML mail to appear, especially given that was achievable with simple modifications to my mutt config.<br /><br />In the next part, I'm going to use the same techniques to investigate why mutt locks up when new LKML mail arrives.</div><div><br /></div></div>
